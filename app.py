import streamlit as st
import re
from streamlit_chat import message

st.set_page_config(
    page_title="Document Q&A VKU Assistant",
    page_icon="ü§ñ",
    layout="wide"
)
    
# Initialize session state variables
if "messages" not in st.session_state:
    st.session_state.messages = []
if "language" not in st.session_state:
    st.session_state.language = "Vietnamese"
if "processed_files" not in st.session_state:
    st.session_state.processed_files = set()
if "current_file_hash" not in st.session_state:
    st.session_state.current_file_hash = None
if "embeding_model" not in st.session_state:
    st.session_state.embedding_model = "SentenceTransformer('keepitreal/vietnamese-sbert')"

from langchain.text_splitter import RecursiveCharacterTextSplitter
import pdfplumber
import docx
import json 
from storage import store_document_chunks, get_relevant_chunks
from config.secretKey import GEMINI_API_KEY
import io
import hashlib
from get_context_online import get_online_context
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from transformers import AutoTokenizer, AutoModel
import torch
from sentence_transformers import SentenceTransformer

# Page configuration


def compute_file_hash(file_content):
    """Compute SHA-256 hash of file content"""
    return hashlib.sha256(file_content).hexdigest()

@st.cache_resource
def load_embedding_models():
    """Load and cache embedding models"""
    #st.session_state.embedding_model = SentenceTransformer("bkai-foundation-models/vietnamese-bi-encoder")
    st.session_state.embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')
    # try:
    #     return {
    #         "English": SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2'),  # Changed to SentenceTransformer
    #         "Vietnamese": SentenceTransformer('keepitreal/vietnamese-sbert')  # Changed to SentenceTransformer
    #     }
    # except Exception as e:
    #     st.error(f"Error loading embedding models: {str(e)}")
    #     return None

@st.cache_resource
def initialize_gemini():
    """Initialize and cache Gemini client"""
    genai.configure(api_key=GEMINI_API_KEY)
    return genai.GenerativeModel('gemini-2.0-flash-lite')

# Load models
embedding_models = load_embedding_models()
model = initialize_gemini()

# Update the model loading section
# if embedding_models:
#     st.session_state.embedding_model = SentenceTransformer('keepitreal/vietnamese-sbert')
# else:
#     st.error("Failed to load embedding models")

def clean_text(text: str) -> str:
    """Clean text by removing unnecessary characters and formatting."""
    # Remove special characters and unnecessary whitespace
    cleaned = re.sub(r'\n+', ' ', text)  # Replace multiple newlines with space
    cleaned = re.sub(r'\s+', ' ', cleaned)  # Replace multiple spaces with single space
    cleaned = re.sub(r'^\d+\.\s*', '', cleaned)  # Remove numbered lists (e.g., "1. ", "2. ")
    cleaned = re.sub(r'[^\w\s.,?!-:]', ' ', cleaned)  # Remove special characters except basic punctuation
    cleaned = re.sub(r'\s+', ' ', cleaned)  # Clean up any resulting multiple spaces
    return cleaned.strip()

import json
import io
import pdfplumber
import docx
import logging
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Configure logging
logging.basicConfig(level=logging.INFO)

def clean_text(text):
    """Helper function to clean text (e.g., remove extra spaces, special characters)"""
    return text.strip()

def clean_text(text):
    """Clean text by removing unwanted characters and extra spaces"""
    # Remove special characters like {}, (), [], etc.
    text = re.sub(r'[{}()\[\],.]', '', text)
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def split_document(document):
    """Split document into chunks with caching"""
    try:
        # Validate input
        if not document or not hasattr(document, 'name'):
            raise ValueError("Invalid document object")

        # Extract file extension
        file_extension = document.name.split('.')[-1].lower()
        allowed_extensions = ['txt', 'pdf', 'docx', 'json']
        if file_extension not in allowed_extensions:
            raise ValueError(f"Unsupported file type: {file_extension}")

        text = ""
        
        # Handle .txt files
        if file_extension == 'txt':
            for encoding in ['utf-8', 'latin-1', 'ascii']:
                try:
                    text = document.getvalue().decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
            if not text:
                raise ValueError("Failed to decode the text file with supported encodings")
            
        # Handle .pdf files
        elif file_extension == 'pdf':
            with pdfplumber.open(io.BytesIO(document.getvalue())) as pdf:
                text = " ".join([page.extract_text() or "" for page in pdf.pages])
            if not text:
                raise ValueError("No text could be extracted from the PDF")
            
        # Handle .docx files
        elif file_extension == 'docx':
            doc = docx.Document(io.BytesIO(document.getvalue()))
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            if not text:
                raise ValueError("No text could be extracted from the DOCX file")
            
        # Handle .json files
        elif file_extension == 'json':
            try:
                json_data = json.loads(document.getvalue().decode('utf-8'))
                # Convert JSON to string representation
                text = json.dumps(json_data, indent=2)
                # Alternative: extract only values if you don't want the structure
                # text = " ".join(str(value) for value in json_data.values() if isinstance(value, (str, int, float, bool)))
            except json.JSONDecodeError as e:
                raise ValueError(f"Invalid JSON file: {str(e)}")
        
        # Validate extracted text
        if not text:
            raise ValueError("No text could be extracted from the document")
        
        # Clean the text before splitting
        text = clean_text(text)
            
        # Split text into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=700,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        chunks = text_splitter.split_text(text)
        
        # Clean chunks before returning
        return [clean_text(chunk) for chunk in chunks]
    
    except Exception as e:
        logging.error(f"Error processing document: {str(e)}")
        return []
# # UI Components
# with st.sidebar:
#     st.title("Configuration")
    
#     Language selection
#     new_language = st.selectbox(
#         "Choose Language",
#         ["English", "Vietnamese"]
#     )
    
#     # Handle language change
#     if new_language != st.session_state.language:
#         st.session_state.language = new_language
#         st.session_state.embedding_model = embedding_models[new_language]
#         # Clear processed files if language changes
#         st.session_state.processed_files = set()
#         st.rerun()
#     else:
#         st.session_state.embedding_model = embedding_models[st.session_state.language]
    
#     st.markdown("---")
#     st.subheader("Model Information")
#     st.info("""
#     - English: MiniLM-L6-v2
#     - Vietnamese: vietnamese-sbert
#     - LLM: llama3.2Q80, llama3.2Q5KM
#     """)

# st.title("üìö Document Q&A Assistant")

# File upload section
col1, col2 = st.columns([2, 1])
with col1:
    uploaded_file = st.file_uploader(
        "Upload your document",
        type=["txt", "pdf", "docx"],
        help="Supported formats: TXT, PDF, DOCX"
    )

# Process uploaded file
if uploaded_file is not None:
    # Compute file hash
    file_content = uploaded_file.getvalue()
    current_hash = compute_file_hash(file_content)
    
    # Check if file needs processing
    if current_hash != st.session_state.current_file_hash:
        with st.spinner("Processing new document..."):
            chunks = split_document(uploaded_file)
            if chunks:
                try:
                    store_document_chunks(chunks)
                    st.session_state.current_file_hash = current_hash   
                    st.session_state.processed_files.add(current_hash)
                    st.success(f"‚ú® Document processed and ready for questions using {st.session_state.language} models")
                except Exception as e:
                    st.error(f"Error storing embeddings: {str(e)}")
    else:
        st.success("‚ú® Document already processed and ready for questions!")

    with col2:
        st.success("‚úÖ File uploaded successfully!")
        st.info(f"üìÑ Filename: {uploaded_file.name}")
else:
    with col2:
        st.warning("‚ö†Ô∏è Please upload a document to begin")

with col2:
    st.session_state.use_internet = st.checkbox(
        "Use Internet for Online Search",
        value=False,
        key="use_internet_toggle"
    )

# Chat interface
st.markdown("---")
st.subheader("üí¨ Chat Interface")

# Display chat history
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat input and response generation
if prompt := st.chat_input("Ask a question..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generate response
    try:
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                # Try to get context from different sources
                context = ""
                
                # First try to get context from database if available
                try:
                    relevant_chunks = get_relevant_chunks(prompt)
                    if relevant_chunks:
                        # Clean each chunk before joining
                        cleaned_chunks = [clean_text(chunk) for chunk in relevant_chunks]
                        context = " ".join(cleaned_chunks)
                except Exception as e:
                    print(f"Database search error: {str(e)}")
                
                # If no context from database and internet is enabled, try online search
                if not context and st.session_state.use_internet:
                    try:
                        context = get_online_context(prompt)
                    except Exception as e:
                        print(f"Online search error: {str(e)}")
                
                # If still no context, proceed with just the question
                if not context:
                    context = "No specific context available. Providing a general response."

                # Prepare prompt based on language
                if st.session_state.language == "English":
                    enhanced_prompt = f"""Based on the following context (if available), provide a comprehensive, accurate, and user-focused answer. If the context is insufficient or unclear, follow the steps below to ensure a helpful response.

                        Context: {context}

                        Question: {prompt}

                        Instructions:
                        - If the context provides sufficient and relevant information, use it to craft a detailed and accurate answer, citing specific details from the context when appropriate.
                        - If the context is limited, unclear, or irrelevant:
                          1. Acknowledge the limitation (e.g., 'The provided context does not fully address this question').
                          2. Provide a general response based on common knowledge or reasonable assumptions relevant to the question (e.g., typical recruitment processes for education-related queries).
                          3. Suggest how the user can refine their question or where they might find more specific information (e.g., 'Please specify the school or check their official website').
                        - Structure the answer clearly and logically, addressing all relevant aspects of the question.
                        - Keep the response concise, easy to understand, and tailored to the user‚Äôs likely needs.
                        - Avoid speculation or inaccurate claims; if uncertain, state this explicitly.
                        - If the question has multiple parts or implications, address each one systematically.

                        Please provide your response:"""
                else:
                    enhanced_prompt = f"""D·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán, ch√≠nh x√°c gi·∫£i th√≠ch m·ªÅm m·∫°i d√†i d√≤ng vƒÉn ch∆∞∆°ng nhi·ªÅu nh·∫•t c√≥ th·ªÉ
                    v√† ph√π h·ª£p v·ªõi m·ª•c ƒë√≠ch t∆∞ v·∫•n tuy·ªÉn sinh. ƒë√¢y l√† ƒë∆∞·ªùng trang tuy·ªÉn sinh c·ªßa tr∆∞·ªùng https://tuyensinh.vku.udn.vn/, h√£y khuy√™n ng∆∞·ªùi d√πng truy c·∫≠p trang n√†y
                    n·∫øu ng·ªØ c·∫£nh ch∆∞a ƒë·∫ßy ƒë·ªß, Ch√∫ √Ω t·ªõi nh·ªØng li√™n k·∫øt ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p, sau ƒë√≥ h√£y x·ª≠ l√Ω theo c√°c b∆∞·ªõc ƒë∆∞·ª£c h∆∞·ªõng d·∫´n d∆∞·ªõi ƒë√¢y.

                        Ng·ªØ c·∫£nh: {context}

                        C√¢u h·ªèi: {prompt}

                        H∆∞·ªõng d·∫´n:
                        - T·∫≠p trung v√†o th√¥ng tin c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
                        - N·∫øu th√¥ng tin trong ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß, tr√≠ch d·∫´n c·ª• th·ªÉ v√† c·∫•u tr√∫c c√¢u tr·∫£ l·ªùi r√µ r√†ng, logic, bao g·ªìm t·∫•t c·∫£ c√°c ƒëi·ªÉm li√™n quan 
                        v√† di·ªÖn d√£i th√™m n·ªôi dung ƒë·ªÉ c√¢u tr·∫£ l·ªùi √Ω nghƒ©a h∆°n, kh√¥ng ƒë·ªÅ c·∫≠p vi·ªác ƒë√£ s·ª≠ d·ª•ng ng·ªØ c·∫£nh ƒë·ªÉ tr·∫£ l·ªùil·ªùi
                        - N·∫øu th√¥ng tin trong ng·ªØ c·∫£nh kh√¥ng ƒë·ªß ho·∫∑c m∆° h·ªì:
                          1. Th·ª´a nh·∫≠n r·∫±ng th√¥ng tin hi·ªán t·∫°i t·ª´ d·ªØ li·ªáu hi·ªán t·∫°it·∫°i kh√¥ng ƒë·∫ßy ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi to√†n di·ªán.
                          2. D·ª±a tr√™n ki·∫øn th·ª©c chung v·ªÅ tuy·ªÉn sinh c·ªßa ri√™ng b·∫°n(v√≠ d·ª•: quy tr√¨nh ƒëƒÉng k√Ω, ti√™u ch√≠ x√©t tuy·ªÉn, l·ªãch tr√¨nh th√¥ng th∆∞·ªùng), ƒë∆∞a ra c√¢u tr·∫£ l·ªùi h·ª£p l√Ω, uy·ªÉn chuy·ªÉn 
                          h∆∞·ªõng ng∆∞·ªùi d√πng t·ªõi https://tuyensinh.vku.udn.vn/ ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n.
                          3. ƒê·ªÅ xu·∫•t ng∆∞·ªùi d√πng cung c·∫•p th√™m chi ti·∫øt ho·∫∑c t√≠ch v√†o √¥ s·ª≠ d·ª•ng Internet ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n.
                        - Tr√°nh ƒë∆∞a ra th√¥ng tin sai l·ªách ho·∫∑c suy ƒëo√°n kh√¥ng c√≥ cƒÉn c·ª©; n·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y n√™u r√µ ƒëi·ªÅu ƒë√≥.
                        - ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi sinh ƒë·ªông, t·ª± nhi√™n, d·ªÖ hi·ªÉu, v√† ph√π h·ª£p v·ªõi nhu c·∫ßu c·ªßa h·ªçc sinh/sinh vi√™n trong b·ªëi c·∫£nh t∆∞ v·∫•n tuy·ªÉn sinh.
                        - N·∫øu c√≥ nhi·ªÅu kh√≠a c·∫°nh li√™n quan trong c√¢u h·ªèi, ph√¢n t√≠ch t·ª´ng kh√≠a c·∫°nh m·ªôt c√°ch c√≥ t·ªï ch·ª©c.

                        Vui l√≤ng cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n m·ªôt c√°ch vƒÉn ch∆∞∆°ng, d√†i nh·∫•t c√≥ th·ªÉ :"""
                
                # Generate response
                print("PROMPT IS: " + enhanced_prompt)
                try:
                    response = model.generate_content(
                        enhanced_prompt,
                        generation_config={
                            'temperature': 0.9,
                            'top_p': 0.2,
                            'max_output_tokens': 8192,
                        },
                        safety_settings={
                            HarmCategory.HARASSMENT: HarmBlockThreshold.LOW,
                            HarmCategory.HATE_SPEECH: HarmBlockThreshold.LOW,
                            HarmCategory.SEXUALLY_EXPLICIT: HarmBlockThreshold.LOW,
                            HarmCategory.DANGEROUS_CONTENT: HarmBlockThreshold.LOW,
                        }
                    )
                except Exception as e:
                    if "HARASSMENT" in str(e):
                        # Try again with more permissive settings
                        response = model.generate_content(
                            enhanced_prompt,
                            safety_settings=None  # Disable safety settings
                        )
                
                assistant_response = response.text
                print("Answer is: "+assistant_response)
                st.markdown(assistant_response)
                
                # Add to chat history
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": assistant_response
                })
    
    except Exception as e:
        st.error(f"Error generating response: {str(e)}")

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
        <small>Built with Streamlit ‚Ä¢ Powered by Nam-Giang</small>
    </div>
    """,
    unsafe_allow_html=True
)